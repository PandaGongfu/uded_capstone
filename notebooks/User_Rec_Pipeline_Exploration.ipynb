{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "Stocktwtis is a platform for active traders to exchange ideas. People post messages and the community can engage by liking or replying to the messages, or they can follow users. In order to cultivate a healthy community, we think it's pivotal to recommend the right people for users to follow. \n",
    "\n",
    "The recommendation process consists of the following steps.\n",
    "1. Find 50 people that a user is closely engaged with in the last 3 months.\n",
    "2. Calculate user reputation score based on an NLP model trained on the text of posted messages.\n",
    "3. Find second-degre  closest connections (i.e. closest connections of the 50 people in step 1), then only recommend those with good reputation scores. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from?What type of information is included? \n",
    "\n",
    "For step 1, we have user activities stored in csv files, such as follow, reply, like and mention.\n",
    "For step 2, messages are in json format along with their metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "LAST_DATE = dt.datetime.now().date().strftime(\"%Y%m%d\")\n",
    "CURRENT_TIME = dt.datetime.now().strftime('%s')\n",
    "\n",
    "S3_BUCKET = 'ds-data-store'\n",
    "S3_PATH = 'graph_data/' + LAST_DATE\n",
    "S3_CLIENT = boto3.client('s3')\n",
    "\n",
    "def read_from_s3(filename):\n",
    "    return S3_CLIENT.get_object(Bucket=S3_BUCKET, Key=S3_PATH + '/{:s}'.format(filename))\n",
    "\n",
    "follow = pd.read_csv(read_from_s3('follow.csv')['Body'], header=None, nrows=3, names=['user_id', 'engaged_with_user_id', 'timestamp'])\n",
    "reply = pd.read_csv(read_from_s3('reply.csv')['Body'], header=None, nrows=3,\n",
    "                       names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp','rel_type'], usecols=[0, 1, 3])\n",
    "mention = pd.read_csv(read_from_s3('mention.csv')['Body'], header=None, nrows=3,\n",
    "                         names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp', 'rel_type'], usecols=[0, 1, 3])\n",
    "like = pd.read_csv(read_from_s3('like.csv')['Body'], header=None, nrows=3, names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp', 'rel_type'],\n",
    "                      usecols=[0, 1, 3])\n",
    "user = pd.read_csv(read_from_s3('user.csv')['Body'], header=None, nrows=3, usecols=[0, 11], names=['user_id', 'suspended'], dtype={'user_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0        1                     3  2009-07-10 22:24:37\n",
      "1        2                     3  2009-07-10 22:27:55\n",
      "2        6                     3  2009-07-12 04:51:15\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0   434018                318908  2019-07-03 00:00:00\n",
      "1  2010747               1148584  2019-07-03 00:00:01\n",
      "2   970851               1474180  2019-07-03 00:00:01\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0   970851                795541  2019-07-03 00:00:01\n",
      "1  1131490               1875300  2019-07-03 00:00:05\n",
      "2  1248076                115915  2019-07-03 00:00:09\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0  1119436               1616220  2019-07-03 00:00:00\n",
      "1  1690933               2152331  2019-07-03 00:00:02\n",
      "2  2141226               2118816  2019-07-03 00:00:03\n",
      "\n",
      "   user_id  suspended\n",
      "0       1          0\n",
      "1       2          0\n",
      "2       3          0\n"
     ]
    }
   ],
   "source": [
    "for df in [follow, reply, mention, like, user]:\n",
    "    print('\\n', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire hose json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    twits = []\n",
    "    for line in open(f, encoding=\"utf-8\"):\n",
    "        twits.append(json.loads(line))\n",
    "    return twits\n",
    "\n",
    "msg_data = read_file('st_messages.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 46555952,\n",
       " 'body': '@BEG007 if u sell and buy back right away then they can put u on free ride violation',\n",
       " 'created_at': '2015-12-09T21:30:39Z',\n",
       " 'source': {'id': 1149,\n",
       "  'title': 'StockTwits for iOS',\n",
       "  'url': 'http://www.stocktwits.com/mobile'},\n",
       " 'conversation': {'parent_message_id': 46550884,\n",
       "  'in_reply_to_message_id': 46555907,\n",
       "  'parent': False,\n",
       "  'replies': 5},\n",
       " 'reshares': {'reshared_count': 0, 'user_ids': []},\n",
       " 'entities': {'sentiment': None},\n",
       " 'user_id': 569057,\n",
       " 'recommended': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# 1. Clean messages using reg exp. \n",
    "# 2. Aggregate messages by user and by week.\n",
    "\n",
    "def msg_level_df(twits):\n",
    "    \"\"\" \n",
    "    Convert raw data to data frame. \n",
    "    \"\"\" \n",
    "    fmt = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "    user = []\n",
    "    recommend = []\n",
    "    body = []\n",
    "    calendar_date = []\n",
    "    link_desc = []\n",
    "\n",
    "    for twit in twits:\n",
    "        user.append(twit['user_id'])\n",
    "        body.append(twit['body'])\n",
    "        calendar_date.append(datetime.date(datetime.strptime(twit['created_at'], fmt)))\n",
    "\n",
    "        desc = ''\n",
    "        if 'links' in twit:\n",
    "            if 'description' in twit['links'][0]:\n",
    "                if(twit['links'][0]['description']!=None):\n",
    "                    desc += twit['links'][0]['description'] \n",
    "        link_desc.append(desc)\n",
    "\n",
    "    df = pd.DataFrame({'user':user, 'calendar_date':calendar_date,'body':body, 'link_desc':link_desc})\n",
    "\n",
    "    # Create weekly bins for aggregating messages\n",
    "    df['daydiff'] = df['calendar_date'].apply(lambda x: (x- date(2015,12,8)).days)\n",
    "    bins = range(0, 190, 7)\n",
    "    df['date_range'] = pd.cut(df['daydiff'], bins)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_twits(s):\n",
    "    \"\"\"\n",
    "    Clean the body of messages with regex.\n",
    "    \"\"\"\n",
    "    regex_user = re.compile('\\@\\w+')\n",
    "    regex_link = re.compile('https?:\\/\\/[^\\s]+')\n",
    "    regex_punctuation = re.compile('[{}]'.format(''.join(['\\\\'+p for p in string.punctuation])))\n",
    "    regex_nonAscii = re.compile('[^\\x00-\\x7F]')\n",
    "    regex_number = re.compile('\\d+')\n",
    "    \n",
    "    s = re.sub(regex_user, '', s)  \n",
    "    s = re.sub(regex_link, 'http', s)    \n",
    "    s = re.sub(regex_punctuation, '', s)    \n",
    "    s = re.sub(regex_nonAscii, '', s)\n",
    "    s = re.sub(regex_number, '', s) \n",
    "    return s.lower()\n",
    "\n",
    "def text_join(x):\n",
    "    return ' '.join(x)\n",
    "\n",
    "def weekly_level_df(df):\n",
    "    \"\"\"\n",
    "    Aggregate messages by user and week.\n",
    "    \"\"\"\n",
    "    txt_df = df[['user','date_range','body','link_desc']].copy()\n",
    "    txt_df['clean_body'] = txt_df['body'].apply(clean_twits)\n",
    "    txt_df['clean_link_desc'] = txt_df['link_desc'].apply(clean_twits)\n",
    "    txt_weekly = txt_df.groupby(['user', 'date_range']).agg({'clean_body':text_join, 'clean_link_desc':text_join}).reset_index()\n",
    "        \n",
    "    return txt_weekly\n",
    "\n",
    "msg_df = msg_level_df(msg_data)\n",
    "msg_weekly = weekly_level_df(msg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>date_range</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>clean_link_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>(0, 7]</td>\n",
       "      <td>why do you have a picture of unabomber on your...</td>\n",
       "      <td>why do you have a picture of the unabomber on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>(7, 14]</td>\n",
       "      <td>clf approaching  low video http stock mkt anal...</td>\n",
       "      <td>be on the lookout for a free webinar lessons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>(14, 21]</td>\n",
       "      <td>very cool end of year video from  http video h...</td>\n",
       "      <td>we look back at the major events of  that inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>(21, 28]</td>\n",
       "      <td>ytd vwaps   spx  spy   day moving average on ...</td>\n",
       "      <td>safeguarding your email address and webinar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>(28, 35]</td>\n",
       "      <td>no trust for spy action but it is interesting ...</td>\n",
       "      <td>stock market analysis   education webinars  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user date_range                                         clean_body  \\\n",
       "0    39     (0, 7]  why do you have a picture of unabomber on your...   \n",
       "1    39    (7, 14]  clf approaching  low video http stock mkt anal...   \n",
       "2    39   (14, 21]  very cool end of year video from  http video h...   \n",
       "3    39   (21, 28]   ytd vwaps   spx  spy   day moving average on ...   \n",
       "4    39   (28, 35]  no trust for spy action but it is interesting ...   \n",
       "\n",
       "                                     clean_link_desc  \n",
       "0  why do you have a picture of the unabomber on ...  \n",
       "1   be on the lookout for a free webinar lessons ...  \n",
       "2  we look back at the major events of  that inve...  \n",
       "3    safeguarding your email address and webinar ...  \n",
       "4    stock market analysis   education webinars  ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_weekly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Model for find_closest_connectios - postgresql table\n",
    "# The closest_connections table has the top 50 users that a user most closely engaged with in the last three months.\n",
    "\n",
    "\n",
    "# Data Model for calc_reputation_score - csv file\n",
    "# The csv file has two columns: user_id and score for each user.\n",
    "\n",
    "\n",
    "# Data Model for find_user_rec - postgresql tables\n",
    "# 1. The recommnedations table contains the recommended users.\n",
    "# 2. For every recommedation, we also store why it is made by looking at each closest connection's contribution. \n",
    "#    The rec_reasons table has that info.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find closest connections from the user engagement files.\n",
    "2. Calculate user reputation scores based on trained model.\n",
    "3. Find second-degree closest connections and make recommendations by combining reputation scores and user weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "# Data Model for find_closest_connectios - postgresql table\n",
    "\"\"\"\n",
    "    CREATE TABLE graph_recs.closest_connections (\n",
    "    user_id             INT    NOT NULL,\n",
    "    closest_connection  INT    NOT NULL,\n",
    "    weight              Decimal     NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, closest_connection)\n",
    "    );\n",
    "\"\"\"  \n",
    "\n",
    "# Data Model for find_user_rec - postgresql tables\n",
    "\"\"\"\n",
    "    CREATE TABLE graph_recs.recommendations (\n",
    "    user_id             INT    NOT NULL,\n",
    "    rec_id              INT    NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, rec_id)\n",
    "    );å\n",
    "    \n",
    "    CREATE TABLE graph_recs.rec_reasons (\n",
    "    user_id             INT    NOT NULL,\n",
    "    rec_id              INT    NOT NULL,\n",
    "    reason_id           INT    NOT NULL,\n",
    "    weight              Decimal    NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, rec_id, reason_id)\n",
    "    );    \n",
    "\"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# Two data quality checks are performed\n",
    "# 1. make sure there is no null data in the results\n",
    "# 2. make sure all results are updated in the last 24 hours.\n",
    "\n",
    "run_quality_checks = DataQualityOperator(\n",
    "    task_id='run_data_quality_checks',\n",
    "    redshift_conn_id='redshift',\n",
    "    test_cases=[\n",
    "        # no null data\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.closest_connections WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.recommendations WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec_reasons WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "\n",
    "        # no expired results\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.closest_connections WHERE updated_at < now()- interval \\'1 day\\'\", 0),       \n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec WHERE updated_at < now()- interval \\'1 day\\'\", 0),   \n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec_reasons WHERE updated_at < now()- interval \\'1 day\\'\", 0),              \n",
    "    ],\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table: closest_connection\n",
    "fields:\n",
    "  - name: user_id\n",
    "    description: id of the user\n",
    "    source: backend database\n",
    "    \n",
    "  - name: closest_connection\n",
    "    description: a user that user_id is closely engaged with\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: weight\n",
    "    description: a time-weighted engagement frequency\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_closest_connections   \n",
    "    \n",
    "csv file: reputation_score\n",
    "fields:\n",
    "  - name: user_id\n",
    "    description: id of the user\n",
    "    source: backend database\n",
    "    \n",
    "  - name: pred\n",
    "    description: a number between 0 and 1 (1 being the highest), indicating how similar that a user is to Stocktwits handpicked good users in terms of choice of words and information quality.\n",
    "    source: calc_reputation_scores\n",
    "    \n",
    "table: recommendations\n",
    "description: user recomendations\n",
    "fields: \n",
    "  - name: user_id\n",
    "    description: id of the user who gets the recommendations\n",
    "    source: backend database\n",
    "    \n",
    "  - name: rec_id\n",
    "    description: id of the user being recommended based on 2nd degree connections of the user and reputation score.   \n",
    "    source: step 3 of the pipeline\n",
    "\n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_user_rec\n",
    "     \n",
    "table: rec_reasons\n",
    "description: reasons for why a user is recommended\n",
    "fields: \n",
    "  - name: user_id\n",
    "    description: id of the user who gets the recommendations\n",
    "    source: backend database\n",
    "    \n",
    "  - name: rec_id\n",
    "    description: id of the user being recommended based on 2nd degree connections of the user and reputation score.   \n",
    "    source: step 3 of the pipeline\n",
    "\n",
    "  - name: reason_id\n",
    "    description: id of a user who is a closest connection of user_id that leads to the recommendation of rec_id\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: weight\n",
    "    description: contribution of reason_id to the recommendation of rec_id\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_user_rec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please refer to readme for the write up."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
