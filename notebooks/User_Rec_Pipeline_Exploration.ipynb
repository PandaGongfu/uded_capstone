{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "Stocktwtis is a platform for active traders to exchange ideas. People post messages and the community can engage by liking or replying to the messages, or they can follow users. In order to cultivate a healthy community, we think it's pivotal to recommend the right people for users to follow. \n",
    "\n",
    "The recommendation process consists of the following steps.\n",
    "1. Find 50 people that a user is closely engaged with in the last 3 months.\n",
    "2. Calculate user reputation score based on an NLP model trained on the text of posted messages.\n",
    "3. Find second-degre  closest connections (i.e. closest connections of the 50 people in step 1), then only recommend those with good reputation scores. \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from?What type of information is included? \n",
    "\n",
    "For step 1, we have user activities stored in csv files, such as follow, reply, like and mention.\n",
    "For step 2, messages are in json format along with their metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "LAST_DATE = dt.datetime.now().date().strftime(\"%Y%m%d\")\n",
    "CURRENT_TIME = dt.datetime.now().strftime('%s')\n",
    "\n",
    "S3_BUCKET = 'ds-data-store'\n",
    "S3_PATH = 'graph_data/' + LAST_DATE\n",
    "S3_CLIENT = boto3.client('s3')\n",
    "\n",
    "def read_from_s3(filename):\n",
    "    return S3_CLIENT.get_object(Bucket=S3_BUCKET, Key=S3_PATH + '/{:s}'.format(filename))\n",
    "\n",
    "follow = pd.read_csv(read_from_s3('follow.csv')['Body'], header=None, nrows=3, names=['user_id', 'engaged_with_user_id', 'timestamp'])\n",
    "reply = pd.read_csv(read_from_s3('reply.csv')['Body'], header=None, nrows=3,\n",
    "                       names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp','rel_type'], usecols=[0, 1, 3])\n",
    "mention = pd.read_csv(read_from_s3('mention.csv')['Body'], header=None, nrows=3,\n",
    "                         names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp', 'rel_type'], usecols=[0, 1, 3])\n",
    "like = pd.read_csv(read_from_s3('like.csv')['Body'], header=None, nrows=3, names=['user_id', 'engaged_with_user_id', 'message_id', 'timestamp', 'rel_type'],\n",
    "                      usecols=[0, 1, 3])\n",
    "user = pd.read_csv(read_from_s3('user.csv')['Body'], header=None, nrows=3, usecols=[0, 11], names=['user_id', 'suspended'], dtype={'user_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0        1                     3  2009-07-10 22:24:37\n",
      "1        2                     3  2009-07-10 22:27:55\n",
      "2        6                     3  2009-07-12 04:51:15\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0  1492089               1187821  2019-06-27 00:00:04\n",
      "1   274150                100060  2019-06-27 00:00:05\n",
      "2  1934590               1201608  2019-06-27 00:00:05\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0  1492089               1192361  2019-06-27 00:00:04\n",
      "1   274150                102367  2019-06-27 00:00:05\n",
      "2  1587535               1411448  2019-06-27 00:00:10\n",
      "\n",
      "    user_id  engaged_with_user_id            timestamp\n",
      "0   937255                277059  2019-06-27 00:00:00\n",
      "1   553849               1358502  2019-06-27 00:00:00\n",
      "2  1587535               1474142  2019-06-27 00:00:02\n",
      "\n",
      "   user_id  suspended\n",
      "0       1          0\n",
      "1       2          0\n",
      "2       3          0\n"
     ]
    }
   ],
   "source": [
    "for df in [follow, reply, mention, like, user]:\n",
    "    print('\\n', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire hose json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    twits = []\n",
    "    for line in open(f, encoding=\"utf-8\"):\n",
    "        twits.append(json.loads(line))\n",
    "    return twits\n",
    "\n",
    "msg_data = read_file('st_messages.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 46555952,\n",
       " 'body': '@BEG007 if u sell and buy back right away then they can put u on free ride violation',\n",
       " 'created_at': '2015-12-09T21:30:39Z',\n",
       " 'source': {'id': 1149,\n",
       "  'title': 'StockTwits for iOS',\n",
       "  'url': 'http://www.stocktwits.com/mobile'},\n",
       " 'conversation': {'parent_message_id': 46550884,\n",
       "  'in_reply_to_message_id': 46555907,\n",
       "  'parent': False,\n",
       "  'replies': 5},\n",
       " 'reshares': {'reshared_count': 0, 'user_ids': []},\n",
       " 'entities': {'sentiment': None},\n",
       " 'user_id': 569057,\n",
       " 'recommended': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# 1. Clean messages using reg exp. \n",
    "# 2. Aggregate messages by user and by week.\n",
    "\n",
    "def msg_level_df(twits):\n",
    "    \"\"\" \n",
    "    Convert raw data to data frame. \n",
    "    \"\"\" \n",
    "    fmt = '%Y-%m-%dT%H:%M:%SZ'\n",
    "\n",
    "    user = []\n",
    "    recommend = []\n",
    "    body = []\n",
    "    calendar_date = []\n",
    "    link_desc = []\n",
    "\n",
    "    for twit in twits:\n",
    "        user.append(twit['user_id'])\n",
    "        body.append(twit['body'])\n",
    "        calendar_date.append(datetime.date(datetime.strptime(twit['created_at'], fmt)))\n",
    "\n",
    "        desc = ''\n",
    "        if 'links' in twit:\n",
    "            if 'description' in twit['links'][0]:\n",
    "                if(twit['links'][0]['description']!=None):\n",
    "                    desc += twit['links'][0]['description'] \n",
    "        link_desc.append(desc)\n",
    "\n",
    "    df = pd.DataFrame({'user':user, 'calendar_date':calendar_date,'body':body, 'link_desc':link_desc})\n",
    "\n",
    "    # Create weekly bins for aggregating messages\n",
    "    df['daydiff'] = df['calendar_date'].apply(lambda x: (x- date(2018,9,9)).days)\n",
    "    bins = range(0, 365, 7)\n",
    "    df['date_range'] = pd.cut(df['daydiff'], bins)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_twits(s):\n",
    "    \"\"\"\n",
    "    Clean the body of messages with regex.\n",
    "    \"\"\"\n",
    "    regex_user = re.compile('\\@\\w+')\n",
    "    regex_link = re.compile('https?:\\/\\/[^\\s]+')\n",
    "    regex_punctuation = re.compile('[{}]'.format(''.join(['\\\\'+p for p in string.punctuation])))\n",
    "    regex_nonAscii = re.compile('[^\\x00-\\x7F]')\n",
    "    regex_number = re.compile('\\d+')\n",
    "    \n",
    "    s = re.sub(regex_user, '', s)  \n",
    "    s = re.sub(regex_link, 'http', s)    \n",
    "    s = re.sub(regex_punctuation, '', s)    \n",
    "    s = re.sub(regex_nonAscii, '', s)\n",
    "    s = re.sub(regex_number, '', s) \n",
    "    return s.lower()\n",
    "\n",
    "def text_join(x):\n",
    "    return ' '.join(x)\n",
    "\n",
    "def weekly_level_df(df):\n",
    "    \"\"\"\n",
    "    Aggregate messages by user and week.\n",
    "    \"\"\"\n",
    "    txt_df = df[['user','date_range','body','link_desc']].copy()\n",
    "    txt_df['clean_body'] = txt_df['body'].apply(clean_twits)\n",
    "    txt_df['clean_link_desc'] = txt_df['link_desc'].apply(clean_twits)\n",
    "    txt_weekly = txt_df.groupby(['user', 'date_range']).agg({'clean_body':text_join, 'clean_link_desc':text_join}).reset_index()\n",
    "        \n",
    "    return txt_weekly\n",
    "\n",
    "msg_df = msg_level_df(msg_data)\n",
    "msg_weekly = weekly_level_df(msg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score Prediction\n",
    "from keras.models import model_from_json\n",
    "\n",
    "max_features = 18900\n",
    "maxlen = 1500\n",
    "\n",
    "def pred_pct(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "def pred_y(x, df, model):\n",
    "    preds = model.predict([x], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_pred = np.zeros(len(preds))\n",
    "    for i in range(len(preds)):\n",
    "        y_pred[i] = 1 if preds[i]>=.5 else 0\n",
    "    \n",
    "    pred_df = pd.concat([df, pd.DataFrame({'pred':y_pred})], axis=1)\n",
    "    pred_grp= pred_df.groupby('user_id').agg({'pred':pred_pct}).reset_index() \n",
    "    pred_grp.sort_values('user_id',inplace=True)\n",
    "\n",
    "    return pred_grp\n",
    "\n",
    "def model_predict(testing_weekly):       \n",
    "    X_test = testing_weekly['clean_body'].values    \n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X_train))\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    x_test = sequence.pad_sequences(X_test, maxlen)\n",
    "    \n",
    "    # Load Trained Model\n",
    "    json_file = open('cnn_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"cnn_model.h5\")\n",
    "    loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])    \n",
    "\n",
    "    return pred_y(x_test, testing_weekly, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Model for find_closest_connectios - postgresql table\n",
    "# The closest_connections table has the top 50 users that a user most closely engaged with in the last three months.\n",
    "\n",
    "\"\"\"\n",
    "    CREATE TABLE graph_recs.closest_connections (\n",
    "    user_id             INT    NOT NULL,\n",
    "    closest_connection  INT    NOT NULL,\n",
    "    weight              Decimal     NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, closest_connection)\n",
    "    );\n",
    "\"\"\"  \n",
    "\n",
    "# Data Model for calc_reputation_score - csv file\n",
    "# The csv file has two columns: user_id and score for each user.\n",
    "\n",
    "\n",
    "# Data Model for find_user_rec - postgresql tables\n",
    "# 1. The recommnedations table contains the recommended users.\n",
    "# 2. For every recommedation, we also store why it is made by looking at each closest connection's contribution. \n",
    "#    The rec_reasons table has that info.  \n",
    "\n",
    "\"\"\"\n",
    "    CREATE TABLE graph_recs.recommendations (\n",
    "    user_id             INT    NOT NULL,\n",
    "    rec_id              INT    NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, rec_id)\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE graph_recs.rec_reasons (\n",
    "    user_id             INT    NOT NULL,\n",
    "    rec_id              INT    NOT NULL,\n",
    "    reason_id           INT    NOT NULL,\n",
    "    weight              Decimal    NOT NULL,\n",
    "    created_at          TIMESTAMPTZ NOT NULL,\n",
    "    updated_at          TIMESTAMPTZ NOT NULL,    \n",
    "    PRIMARY KEY         (user_id, rec_id, reason_id)\n",
    "    );    \n",
    "\"\"\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find closest connections from the user engagement files.\n",
    "2. Calculate user reputation scores based on trained model.\n",
    "3. Find second-degree closest connections and make recommendations by combining reputation scores and user weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "# Please refer to the repo for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "run_quality_checks = DataQualityOperator(\n",
    "    task_id='run_data_quality_checks',\n",
    "    redshift_conn_id='redshift',\n",
    "    test_cases=[\n",
    "        # no null data\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.closest_connections WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.recommendations WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec_reasons WHERE user_id IS NULL OR rec_id IS NULL\", 0),\n",
    "\n",
    "        # no expired results\n",
    "        (\"SELECT COUNT(user_id) from graph_recs.closest_connections WHERE updated_at < now()- interval \\'1 day\\'\", 0),       \n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec WHERE updated_at < now()- interval \\'1 day\\'\", 0),   \n",
    "        (\"SELECT COUNT(user_id) from graph_recs.user_rec_reasons WHERE updated_at < now()- interval \\'1 day\\'\", 0),              \n",
    "    ],\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "redshift = PostgresHook(postgres_conn_id=\"redshift\")\n",
    "for (test_query, expected_result)  in self.test_cases:\n",
    "    self.log.info(f'Running Data Quality Check: {test_query}')\n",
    "    records = redshift.get_records(test_query)\n",
    "\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        self.log.info(f\"Data quality check failed. {test_query} returned no results\")\n",
    "        raise ValueError(f\"Data quality check failed. {test_query} returned no results\")\n",
    "\n",
    "    num_records = records[0][0]\n",
    "    if num_records != expected_result:\n",
    "        self.log.info(f\"Data quality check failed. {test_query} contained {num_records} rows but expected {expected_result}\")\n",
    "        raise ValueError(f\"Data quality check failed. {test_query} contained {num_records} rows but expected {expected_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "table: closest_connection\n",
    "fields:\n",
    "  - name: user_id\n",
    "    description: id of the user\n",
    "    source: backend database\n",
    "    \n",
    "  - name: closest_connection\n",
    "    description: a user that user_id is closely engaged with\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: weight\n",
    "    description: a time-weighted engagement frequency\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 1 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_closest_connections   \n",
    "    \n",
    "csv file: reputation_score\n",
    "fields:\n",
    "  - name: user_id\n",
    "    description: id of the user\n",
    "    source: backend database\n",
    "    \n",
    "  - name: pred\n",
    "    description: a number between 0 and 1 (1 being the highest), indicating how similar that a user is to Stocktwits handpicked good users in terms of choice of words and information quality.\n",
    "    source: calc_reputation_scores\n",
    "    \n",
    "table: recommendations\n",
    "description: user recomendations\n",
    "fields: \n",
    "  - name: user_id\n",
    "    description: id of the user who gets the recommendations\n",
    "    source: backend database\n",
    "    \n",
    "  - name: rec_id\n",
    "    description: id of the user being recommended based on 2nd degree connections of the user and reputation score.   \n",
    "    source: step 3 of the pipeline\n",
    "\n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_user_rec\n",
    "     \n",
    "table: rec_reasons\n",
    "description: reasons for why a user is recommended\n",
    "fields: \n",
    "  - name: user_id\n",
    "    description: id of the user who gets the recommendations\n",
    "    source: backend database\n",
    "    \n",
    "  - name: rec_id\n",
    "    description: id of the user being recommended based on 2nd degree connections of the user and reputation score.   \n",
    "    source: step 3 of the pipeline\n",
    "\n",
    "  - name: reason_id\n",
    "    description: id of a user who is a closest connection of user_id that leads to the recommendation of rec_id\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: weight\n",
    "    description: contribution of reason_id to the recommendation of rec_id\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: created_at \n",
    "    description: the timestamp that the record was initially created\n",
    "    source: step 3 of the pipeline\n",
    "    \n",
    "  - name: updated_at\n",
    "    description: the timestamp that the record was updated\n",
    "    source: find_user_rec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please refer to readme for the write up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "LAST_DATE = dt.datetime.now().date().strftime(\"%Y%m%d\")\n",
    "CURRENT_TIME = dt.datetime.now().strftime('%s')\n",
    "filename = 't.csv'\n",
    "S3_BUCKET = 'ds-data-store'\n",
    "S3_PATH = 'graph_data/' + LAST_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ds-data-store/graph_data/20190926/t.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('s3://',S3_BUCKET, S3_PATH, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_rec</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  user_rec  score\n",
       "0        2         2    0.6\n",
       "1        3         5    0.8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=pd.DataFrame({'user_id':[1,2,3], 'user_rec':[1,2,5]})\n",
    "bb=pd.DataFrame({'user_id':[1,2,3], 'score':[0.1,0.6,0.8]})\n",
    "pd.merge(aa, bb[bb['score']>0.5], left_on='user_id', right_on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
